{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shokoufehnaseri/Text-Mining/blob/main/text_mining_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Sentiment analysis of tweet data**\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ***1. Introduction***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sentiment analysis, a key technique in natural language processing (NLP), has gained substantial attention for its ability to analyze and interpret opinions, emotions, and attitudes expressed in textual data. In recent years, the development of transformer models, such as FinBERT, has revolutionized the field of sentiment analysis, particularly in the context of financial data. These models, built on deep learning architectures like BERT, have demonstrated a remarkable ability to understand the nuances of language and context, making them particularly effective for analyzing specialized datasets such as financial news and social media content.\n",
        "\n",
        "This project explores the performance of modern transformer models, specifically FinBERT, in comparison with traditional sentiment analysis methods like Support Vector Machines (SVM). While SVM has long been a popular choice for text classification tasks, its reliance on manual feature extraction and limited ability to capture contextual information poses certain challenges. In contrast, transformer models, with their self-attention mechanism, excel in capturing context and meaning from large amounts of unstructured data.\n",
        "\n",
        "The dataset used for this study consists of tweets related to the S&P 500 index, a stock market index tracking the 500 largest publicly traded companies in the United States. Social media platforms like Twitter have become a rich source of real-time information, with users frequently sharing their opinions and reactions to market events. By analyzing these tweets, this project aims to uncover insights into market sentiment and compare the effectiveness of modern and traditional sentiment analysis techniques\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImuCnD_JGgM_"
      },
      "source": [
        "## ***2. Data***\n",
        "\n",
        "**Data Description:**\n",
        "\n",
        "The Twitter raw data was downloaded using the Twitter REST API search, specifically the \"Tweepy (version 3.8.0)\" Python package, which simplifies the interaction between the REST API and developers. The Twitter REST API retrieves data from the past seven days and allows filtering by language. The tweets were filtered for the English (en) language.\n",
        "\n",
        "Data collection was performed from April 9 to July 16, 2020, using the following Twitter tags as search parameters: #SPX500, #SP500, SPX500, SP500, $SPX, #stocks, $MSFT, $AAPL, $AMZN, $FB, $BBRK.B, $GOOG, $JNJ, $JPM, $V, $PG, $MA, $INTC, $UNH, $BAC, $T, $HD, $XOM, $DIS, $VZ, $KO, $MRK, $CMCSA, $CVX, $PEP, $PFE. Due to the large volume of data, I stored only each tweet's content and creation date.\n",
        "\n",
        "The file tweets_labelled_09042020_16072020.csv consists of 5,000 tweets selected using random sampling from a total of 943,672. Of these, 1,300 tweets were manually annotated and reviewed by a second independent annotator. The file tweets_remaining_09042020_16072020.csv contains the remaining 938,672 tweets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer, ENGLISH_STOP_WORDS\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from nltk import word_tokenize\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***Loading Dataset*** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "tweet_data = pd.read_csv(r\"C:\\Users\\Shokoufeh\\OneDrive\\Thesis\\thesis_coding\\text_mining_project\\Text-Mining\\tweets_labelled_09042020_16072020.csv\", delimiter=\";\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yGR033MG5tb"
      },
      "source": [
        "**Inspect the Data**\n",
        "\n",
        "Check for issues like missing values or incorrectly formatted columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6b4VnJYG9FI",
        "outputId": "148c2ed6-bd81-48a3-df3f-3104fab09d13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       id                 created_at  \\\n",
            "0   77522  2020-04-15 01:03:46+00:00   \n",
            "1  661634  2020-06-25 06:20:06+00:00   \n",
            "2  413231  2020-06-04 15:41:45+00:00   \n",
            "3  760262  2020-07-03 19:39:35+00:00   \n",
            "4  830153  2020-07-09 14:39:14+00:00   \n",
            "\n",
            "                                                text sentiment  \n",
            "0  RT @RobertBeadles: YoðŸ’¥\\nEnter to WIN 1,000 Mon...  positive  \n",
            "1  #SriLanka surcharge on fuel removed!\\nâ›½ðŸ“‰\\nThe ...  negative  \n",
            "2  Net issuance increases to fund fiscal programs...  positive  \n",
            "3  RT @bentboolean: How much of Amazon's traffic ...  positive  \n",
            "4  $AMD Ryzen 4000 desktop CPUs looking â€˜greatâ€™ a...  positive  \n"
          ]
        }
      ],
      "source": [
        "print(tweet_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5000 entries, 0 to 4999\n",
            "Data columns (total 4 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   id          5000 non-null   int64 \n",
            " 1   created_at  5000 non-null   object\n",
            " 2   text        5000 non-null   object\n",
            " 3   sentiment   1300 non-null   object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 156.4+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(tweet_data.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "sentiment\n",
              "positive    528\n",
              "neutral     424\n",
              "negative    348\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tweet_data['sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "id               0\n",
            "created_at       0\n",
            "text             0\n",
            "sentiment     3700\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(tweet_data.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset consists of 5,000 observations. Among these, 1,300 observations are labeled with sentiments as 'positive', 'negative', or 'neutral', while the remaining 3,700 observations are unlabeled, with their sentiment marked as `NaN`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Clean the Text Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    RT @RobertBeadles: YoðŸ’¥\\nEnter to WIN 1,000 Mon...\n",
            "1    #SriLanka surcharge on fuel removed!\\nâ›½ðŸ“‰\\nThe ...\n",
            "2    Net issuance increases to fund fiscal programs...\n",
            "3    RT @bentboolean: How much of Amazon's traffic ...\n",
            "4    $AMD Ryzen 4000 desktop CPUs looking â€˜greatâ€™ a...\n",
            "Name: text, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(tweet_data['text'].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGIC0pO6ISaQ"
      },
      "source": [
        "\n",
        "As observed, raw text data often contains noise and inconsistencies that can impede accurate sentiment analysis. To address this, preprocessing is an essential step to clean, standardize, and structure the data, ensuring its suitability for machine learning algorithms. The following steps are commonly employed to prepare textual data effectively:\n",
        "\n",
        "*Lowercasing:* All text is converted to lowercase to ensure uniformity and avoid treating the same word differently due to capitalization (e.g., \"Happy\" vs. \"happy\").\n",
        "\n",
        "*Removal of URLs:* Text often contains hyperlinks that do not contribute to the sentiment of the content. These are removed to reduce noise.\n",
        "\n",
        "*Remove Mentions:* Mentions, typically denoted by the @ symbol followed by a username (e.g., @user), are common in social media text. While they indicate a reference to another user, they usually do not contribute to the sentiment of the text and are removed to reduce noise.\n",
        "\n",
        "*Handling Hashtags:* Hashtags are common in social media text. While the # symbol is removed, the associated words are retained, as they may provide context or sentiment-related information.\n",
        "\n",
        "*Removal of Numeric and Punctuation Data:* Numbers and punctuation marks, unless contextually relevant, are removed to simplify the text.\n",
        "\n",
        "Perform text preprocessing to ensure consistency and remove noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import preprocess_tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Shokoufeh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\strings\\object_array.py:172: FutureWarning: Possible nested set at position 1\n",
            "  pat = re.compile(pat, flags=flags)\n"
          ]
        }
      ],
      "source": [
        "tweet_data = preprocess_tweet.Preprocess_Tweets(tweet_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>created_at</th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>Text_Cleaned</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>77522</td>\n",
              "      <td>2020-04-15 01:03:46+00:00</td>\n",
              "      <td>RT @RobertBeadles: YoðŸ’¥\\nEnter to WIN 1,000 Mon...</td>\n",
              "      <td>positive</td>\n",
              "      <td>rt robertbeadles yo enter to win monarch token...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>661634</td>\n",
              "      <td>2020-06-25 06:20:06+00:00</td>\n",
              "      <td>#SriLanka surcharge on fuel removed!\\nâ›½ðŸ“‰\\nThe ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>srilanka surcharge on fuel removed the surchar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>413231</td>\n",
              "      <td>2020-06-04 15:41:45+00:00</td>\n",
              "      <td>Net issuance increases to fund fiscal programs...</td>\n",
              "      <td>positive</td>\n",
              "      <td>net issuance increases to fund fiscal programs...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>760262</td>\n",
              "      <td>2020-07-03 19:39:35+00:00</td>\n",
              "      <td>RT @bentboolean: How much of Amazon's traffic ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>rt bentboolean how much of amazons traffic is ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>830153</td>\n",
              "      <td>2020-07-09 14:39:14+00:00</td>\n",
              "      <td>$AMD Ryzen 4000 desktop CPUs looking â€˜greatâ€™ a...</td>\n",
              "      <td>positive</td>\n",
              "      <td>amd ryzen desktop cpus looking great and on tr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id                 created_at  \\\n",
              "0   77522  2020-04-15 01:03:46+00:00   \n",
              "1  661634  2020-06-25 06:20:06+00:00   \n",
              "2  413231  2020-06-04 15:41:45+00:00   \n",
              "3  760262  2020-07-03 19:39:35+00:00   \n",
              "4  830153  2020-07-09 14:39:14+00:00   \n",
              "\n",
              "                                                text sentiment  \\\n",
              "0  RT @RobertBeadles: YoðŸ’¥\\nEnter to WIN 1,000 Mon...  positive   \n",
              "1  #SriLanka surcharge on fuel removed!\\nâ›½ðŸ“‰\\nThe ...  negative   \n",
              "2  Net issuance increases to fund fiscal programs...  positive   \n",
              "3  RT @bentboolean: How much of Amazon's traffic ...  positive   \n",
              "4  $AMD Ryzen 4000 desktop CPUs looking â€˜greatâ€™ a...  positive   \n",
              "\n",
              "                                        Text_Cleaned  \n",
              "0  rt robertbeadles yo enter to win monarch token...  \n",
              "1  srilanka surcharge on fuel removed the surchar...  \n",
              "2  net issuance increases to fund fiscal programs...  \n",
              "3  rt bentboolean how much of amazons traffic is ...  \n",
              "4  amd ryzen desktop cpus looking great and on tr...  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tweet_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Remove Stop Words**\n",
        "Remove common words that don't contribute to sentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# General English stop words\n",
        "general_stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Define custom stopwords\n",
        "custom_stopwords = set([\n",
        "    'SP500', 'S&P', '500', 'index', 'stock', 'market', 'stocks',\n",
        "    'trading', 'finance', 'investing', 'investor', 'business',\n",
        "    \"billion\", 'price', 'StockMarket', 'share',\n",
        "    'RT', 'http','dollar','dollars', 'percent', 'https', 'www', 'bit.ly', '@username', '#finance',\n",
        "    'breaking', 'update', 'today', 'yesterday', 'tomorrow',  \"aapl\", \"msft\", \"amzn\", \"tsla\", \"googl\", \"meta\", \"nvda\", \"brk.b\", \"jnj\", \"pg\", \n",
        "    \"v\", \"unh\", \"hd\", \"ma\", \"pep\", \"bac\", \"xom\", \"ko\", \"abbv\", \"avgo\", \"cost\", \n",
        "    \"mcd\", \"csco\", \"pfe\", \"cvx\", \"adbe\", \"mrk\", \"nflx\", \"dis\", \"intc\", \"wmt\", \n",
        "    \"tmo\", \"orcl\", \"crm\", \"nke\", \"wfc\", \"acn\", \"lin\", \"mdt\", \"txn\", \"dhr\", \"hon\", \n",
        "    \"lly\", \"vz\", \"schw\", \"amgn\", \"ibm\", \"t\", \"qcom\", \"sbux\", \"mmm\", \"gs\", \"rtx\", \n",
        "    \"ups\", \"low\", \"bmy\", \"cat\", \"spgi\", \"isrg\", \"c\", \"elv\", \"lmt\", \"mo\", \"bkng\", \n",
        "    \"adp\", \"amd\", \"de\", \"pm\", \"gild\", \"syk\", \"ge\", \"amt\", \"ms\", \"blk\", \"cci\", \n",
        "    \"cvs\", \"now\", \"intu\", \"ci\", \"zts\", \"eqix\", \"ice\", \"tgt\", \"mu\", \"fis\", \"ew\", \n",
        "    \"cb\", \"mmc\", \"apd\", \"cl\", \"so\", \"pgr\", \"duke\", \"pld\", \"aon\", \"fisv\", \"itw\", \n",
        "    \"stz\", \"regn\", \"adi\", \"hum\", \"exc\", \"pxd\", \"snps\", \"cop\", \"kdp\", \"kmb\", \"rop\", \n",
        "    \"etn\", \"aep\", \"eog\", \"mar\", \"atvi\", \"noc\", \"pru\", \"oxy\", \"orly\", \"d\", \"chrw\", \n",
        "    \"bax\", \"adm\", \"fdx\", \"aig\", \"dg\", \"tsco\", \"qqq\", \"fb\", \"spx\", \"spy\",\"new\", \"day\", \"week\", \"rt\"\n",
        "])\n",
        "\n",
        "# Combine general_stop_words with custom stopwords\n",
        "combined_stopwords = general_stop_words.union(custom_stopwords)\n",
        "\n",
        "# Remove stopwords function\n",
        "def remove_stopwords(text, stopwords):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in stopwords]\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "# Apply the function to the 'text' column\n",
        "tweet_data['Text_Cleaned'] = tweet_data['Text_Cleaned'].apply(lambda x: remove_stopwords(x, combined_stopwords))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eFP0g1wIuOH"
      },
      "source": [
        "### **Tokenize the Text**\n",
        "Tokenization is the process of splitting text into smaller units, called tokens, which are often individual words. This step is essential for text preprocessing as it enables the analysis of each word separately. For example, the sentence \"I love programming!\" would be tokenized into ['I', 'love', 'programming', '!'].\n",
        "\n",
        "In Python, the word_tokenize function from the nltk library is commonly used for this purpose. It efficiently breaks a sentence into tokens, taking care of punctuation and special characters, allowing for precise text analysis.\n",
        "\n",
        "Split the cleaned text into individual words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8LCGBuXItPI",
        "outputId": "9614d409-611c-4593-cbb8-1cca7c82d790"
      },
      "outputs": [],
      "source": [
        "\n",
        "tweet_data['tokens'] = tweet_data['Text_Cleaned'].apply(word_tokenize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code calculates the number of words in each entry of the dataset and identifies the minimum and maximum word counts. This analysis provides a better understanding of the text length distribution, offering valuable insights into the variability of the data prior to further processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 42\n"
          ]
        }
      ],
      "source": [
        "tweet_data['n_word'] = [len(str(row['tokens']).split()) for _, row in tweet_data.iterrows()]\n",
        "\n",
        "print(min(tweet_data['n_word']), \n",
        "max(tweet_data['n_word']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>created_at</th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>Text_Cleaned</th>\n",
              "      <th>tokens</th>\n",
              "      <th>n_word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>77522</td>\n",
              "      <td>2020-04-15 01:03:46+00:00</td>\n",
              "      <td>RT @RobertBeadles: YoðŸ’¥\\nEnter to WIN 1,000 Mon...</td>\n",
              "      <td>positive</td>\n",
              "      <td>robertbeadles yo enter win monarch tokens us c...</td>\n",
              "      <td>[robertbeadles, yo, enter, win, monarch, token...</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>661634</td>\n",
              "      <td>2020-06-25 06:20:06+00:00</td>\n",
              "      <td>#SriLanka surcharge on fuel removed!\\nâ›½ðŸ“‰\\nThe ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>srilanka surcharge fuel removed surcharge rs i...</td>\n",
              "      <td>[srilanka, surcharge, fuel, removed, surcharge...</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>413231</td>\n",
              "      <td>2020-06-04 15:41:45+00:00</td>\n",
              "      <td>Net issuance increases to fund fiscal programs...</td>\n",
              "      <td>positive</td>\n",
              "      <td>net issuance increases fund fiscal programs gt...</td>\n",
              "      <td>[net, issuance, increases, fund, fiscal, progr...</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>760262</td>\n",
              "      <td>2020-07-03 19:39:35+00:00</td>\n",
              "      <td>RT @bentboolean: How much of Amazon's traffic ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>bentboolean much amazons traffic served fastly...</td>\n",
              "      <td>[bentboolean, much, amazons, traffic, served, ...</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>830153</td>\n",
              "      <td>2020-07-09 14:39:14+00:00</td>\n",
              "      <td>$AMD Ryzen 4000 desktop CPUs looking â€˜greatâ€™ a...</td>\n",
              "      <td>positive</td>\n",
              "      <td>ryzen desktop cpus looking great track launch ...</td>\n",
              "      <td>[ryzen, desktop, cpus, looking, great, track, ...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id                 created_at  \\\n",
              "0   77522  2020-04-15 01:03:46+00:00   \n",
              "1  661634  2020-06-25 06:20:06+00:00   \n",
              "2  413231  2020-06-04 15:41:45+00:00   \n",
              "3  760262  2020-07-03 19:39:35+00:00   \n",
              "4  830153  2020-07-09 14:39:14+00:00   \n",
              "\n",
              "                                                text sentiment  \\\n",
              "0  RT @RobertBeadles: YoðŸ’¥\\nEnter to WIN 1,000 Mon...  positive   \n",
              "1  #SriLanka surcharge on fuel removed!\\nâ›½ðŸ“‰\\nThe ...  negative   \n",
              "2  Net issuance increases to fund fiscal programs...  positive   \n",
              "3  RT @bentboolean: How much of Amazon's traffic ...  positive   \n",
              "4  $AMD Ryzen 4000 desktop CPUs looking â€˜greatâ€™ a...  positive   \n",
              "\n",
              "                                        Text_Cleaned  \\\n",
              "0  robertbeadles yo enter win monarch tokens us c...   \n",
              "1  srilanka surcharge fuel removed surcharge rs i...   \n",
              "2  net issuance increases fund fiscal programs gt...   \n",
              "3  bentboolean much amazons traffic served fastly...   \n",
              "4  ryzen desktop cpus looking great track launch ...   \n",
              "\n",
              "                                              tokens  n_word  \n",
              "0  [robertbeadles, yo, enter, win, monarch, token...      14  \n",
              "1  [srilanka, surcharge, fuel, removed, surcharge...      26  \n",
              "2  [net, issuance, increases, fund, fiscal, progr...      25  \n",
              "3  [bentboolean, much, amazons, traffic, served, ...      14  \n",
              "4  [ryzen, desktop, cpus, looking, great, track, ...      10  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tweet_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Vader sentiment analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     C:\\Users\\Shokoufeh\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "\n",
        "# Download VADER lexicon\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Initialize VADER sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "\n",
        "# Function to apply VADER sentiment analysis\n",
        "def analyze_sentiment(text):\n",
        "    scores = sia.polarity_scores(text)\n",
        "    # Return compound score and label\n",
        "    compound_score = scores[\"compound\"]\n",
        "    if compound_score >= 0.05:\n",
        "        sentiment_label = \"positive\"\n",
        "    elif compound_score <= -0.05:\n",
        "        sentiment_label = \"negative\"\n",
        "    else:\n",
        "        sentiment_label = \"neutral\"\n",
        "    return pd.Series([compound_score, sentiment_label])\n",
        "\n",
        "# Apply VADER sentiment analysis to cleaned_text\n",
        "tweet_data[[\"vader_score\", \"vader_sentiment\"]] = tweet_data[\"Text_Cleaned\"].apply(analyze_sentiment)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0       positive\n",
            "1       positive\n",
            "2       positive\n",
            "3       positive\n",
            "4       positive\n",
            "          ...   \n",
            "4995    positive\n",
            "4996    positive\n",
            "4997     neutral\n",
            "4998    positive\n",
            "4999    positive\n",
            "Name: vader_sentiment, Length: 5000, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Display results\n",
        "print(tweet_data[\"vader_sentiment\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split into labeled and unlabeled data\n",
        "labeled_data = tweet_data[tweet_data[\"sentiment\"].notna()]\n",
        "unlabeled_data = tweet_data[tweet_data[\"sentiment\"].isna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 68.69%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Shokoufeh\\AppData\\Local\\Temp\\ipykernel_6024\\3118178342.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  labeled_data[\"comparison\"] = labeled_data[\"sentiment\"] == labeled_data[\"vader_sentiment\"]\n"
          ]
        }
      ],
      "source": [
        "# Add a new column to indicate whether the sentiments match\n",
        "labeled_data[\"comparison\"] = labeled_data[\"sentiment\"] == labeled_data[\"vader_sentiment\"]\n",
        "\n",
        "# Calculate the number of matches\n",
        "matches = labeled_data[\"comparison\"].sum()\n",
        "\n",
        "# Calculate total number of rows\n",
        "total = len(labeled_data)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = (matches / total) * 100\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZoZPEoWKUft"
      },
      "source": [
        "### **6. Lemmatization or Stemming**\n",
        "Reduce words to their base forms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>created_at</th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>Text_Cleaned</th>\n",
              "      <th>tokens</th>\n",
              "      <th>n_word</th>\n",
              "      <th>vader_score</th>\n",
              "      <th>vader_sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>77522</td>\n",
              "      <td>2020-04-15 01:03:46+00:00</td>\n",
              "      <td>RT @RobertBeadles: YoðŸ’¥\\nEnter to WIN 1,000 Mon...</td>\n",
              "      <td>positive</td>\n",
              "      <td>robertbeadles yo enter win monarch tokens us c...</td>\n",
              "      <td>[robertbeadles, yo, enter, win, monarch, token...</td>\n",
              "      <td>14</td>\n",
              "      <td>0.5859</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>661634</td>\n",
              "      <td>2020-06-25 06:20:06+00:00</td>\n",
              "      <td>#SriLanka surcharge on fuel removed!\\nâ›½ðŸ“‰\\nThe ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>srilanka surcharge fuel removed surcharge rs i...</td>\n",
              "      <td>[srilanka, surcharge, fuel, removed, surcharge...</td>\n",
              "      <td>26</td>\n",
              "      <td>0.2023</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>413231</td>\n",
              "      <td>2020-06-04 15:41:45+00:00</td>\n",
              "      <td>Net issuance increases to fund fiscal programs...</td>\n",
              "      <td>positive</td>\n",
              "      <td>net issuance increases fund fiscal programs gt...</td>\n",
              "      <td>[net, issuance, increases, fund, fiscal, progr...</td>\n",
              "      <td>25</td>\n",
              "      <td>0.0516</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>760262</td>\n",
              "      <td>2020-07-03 19:39:35+00:00</td>\n",
              "      <td>RT @bentboolean: How much of Amazon's traffic ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>bentboolean much amazons traffic served fastly...</td>\n",
              "      <td>[bentboolean, much, amazons, traffic, served, ...</td>\n",
              "      <td>14</td>\n",
              "      <td>0.3818</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>830153</td>\n",
              "      <td>2020-07-09 14:39:14+00:00</td>\n",
              "      <td>$AMD Ryzen 4000 desktop CPUs looking â€˜greatâ€™ a...</td>\n",
              "      <td>positive</td>\n",
              "      <td>ryzen desktop cpus looking great track launch ...</td>\n",
              "      <td>[ryzen, desktop, cpus, looking, great, track, ...</td>\n",
              "      <td>10</td>\n",
              "      <td>0.6249</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id                 created_at  \\\n",
              "0   77522  2020-04-15 01:03:46+00:00   \n",
              "1  661634  2020-06-25 06:20:06+00:00   \n",
              "2  413231  2020-06-04 15:41:45+00:00   \n",
              "3  760262  2020-07-03 19:39:35+00:00   \n",
              "4  830153  2020-07-09 14:39:14+00:00   \n",
              "\n",
              "                                                text sentiment  \\\n",
              "0  RT @RobertBeadles: YoðŸ’¥\\nEnter to WIN 1,000 Mon...  positive   \n",
              "1  #SriLanka surcharge on fuel removed!\\nâ›½ðŸ“‰\\nThe ...  negative   \n",
              "2  Net issuance increases to fund fiscal programs...  positive   \n",
              "3  RT @bentboolean: How much of Amazon's traffic ...  positive   \n",
              "4  $AMD Ryzen 4000 desktop CPUs looking â€˜greatâ€™ a...  positive   \n",
              "\n",
              "                                        Text_Cleaned  \\\n",
              "0  robertbeadles yo enter win monarch tokens us c...   \n",
              "1  srilanka surcharge fuel removed surcharge rs i...   \n",
              "2  net issuance increases fund fiscal programs gt...   \n",
              "3  bentboolean much amazons traffic served fastly...   \n",
              "4  ryzen desktop cpus looking great track launch ...   \n",
              "\n",
              "                                              tokens  n_word  vader_score  \\\n",
              "0  [robertbeadles, yo, enter, win, monarch, token...      14       0.5859   \n",
              "1  [srilanka, surcharge, fuel, removed, surcharge...      26       0.2023   \n",
              "2  [net, issuance, increases, fund, fiscal, progr...      25       0.0516   \n",
              "3  [bentboolean, much, amazons, traffic, served, ...      14       0.3818   \n",
              "4  [ryzen, desktop, cpus, looking, great, track, ...      10       0.6249   \n",
              "\n",
              "  vader_sentiment  \n",
              "0        positive  \n",
              "1        positive  \n",
              "2        positive  \n",
              "3        positive  \n",
              "4        positive  "
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tweet_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **7. Encode Sentiment Labels**\n",
        "Convert sentiment labels (e.g., \"positive\", \"negative\") into numeric values for machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "tweet_data['sentiment_encoded'] = label_encoder.fit_transform(tweet_data['vader_sentiment'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ9O2wWbKwpR"
      },
      "source": [
        "### **8. Prepare Data for Modeling**\n",
        "Split the data into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "GYyIo_NLK2Gv"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = tweet_data['Text_Cleaned']\n",
        "y = tweet_data['sentiment_encoded']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXbipgj8LFpc"
      },
      "source": [
        "### **9. Vectorize Text Data**\n",
        "Convert text into numerical form using techniques like TF-IDF or Count Vectorization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "#X_train = [\" \".join(doc) if isinstance(doc, list) else doc for doc in X_train]\n",
        "#X_test = [\" \".join(doc) if isinstance(doc, list) else doc for doc in X_test]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "3STIhIrdLLto"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHxuTFlwLWL2"
      },
      "source": [
        "### **10. Ready for Sentiment Analysis**\n",
        "You can now use your processed data with machine learning models or sentiment analysis tools.\n",
        "\n",
        "For example, using a logistic regression model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh_83ZvfLYt1",
        "outputId": "727ed4f4-0af6-40d2-d0ca-41269e541e1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.21      0.34       156\n",
            "           1       0.70      0.81      0.75       380\n",
            "           2       0.76      0.85      0.80       464\n",
            "\n",
            "    accuracy                           0.74      1000\n",
            "   macro avg       0.77      0.62      0.63      1000\n",
            "weighted avg       0.75      0.74      0.71      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      Predicted  Actual\n",
            "1501          1       1\n",
            "2586          1       1\n",
            "2653          2       2\n",
            "1055          2       1\n",
            "705           2       2\n",
            "...         ...     ...\n",
            "4711          2       2\n",
            "2313          2       2\n",
            "3214          2       2\n",
            "2732          2       2\n",
            "1926          1       2\n",
            "\n",
            "[1000 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "# Create a DataFrame\n",
        "df = pd.DataFrame({'Predicted': y_pred, 'Actual': y_test})\n",
        "\n",
        "# Print the DataFrame as a table\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Vader classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     C:\\Users\\Shokoufeh\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***SVM Classifier***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.35      0.47       156\n",
            "           1       0.71      0.85      0.77       380\n",
            "           2       0.82      0.83      0.83       464\n",
            "\n",
            "    accuracy                           0.76      1000\n",
            "   macro avg       0.76      0.68      0.69      1000\n",
            "weighted avg       0.76      0.76      0.75      1000\n",
            "\n",
            "SVM Accuracy: 0.762\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Train SVM classifier\n",
        "svm_clf = SVC(kernel='linear', random_state=42)\n",
        "svm_clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_svm = svm_clf.predict(X_test_tfidf)\n",
        "print(\"SVM Classification Report:\\n\", classification_report(y_test, y_pred_svm))\n",
        "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  ***Naive Bayes Classifier***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.08      0.15       156\n",
            "           1       0.71      0.59      0.65       380\n",
            "           2       0.62      0.90      0.74       464\n",
            "\n",
            "    accuracy                           0.66      1000\n",
            "   macro avg       0.78      0.52      0.51      1000\n",
            "weighted avg       0.71      0.66      0.61      1000\n",
            "\n",
            "Naive Bayes Accuracy: 0.655\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Train Naive Bayes classifier\n",
        "nb_clf = MultinomialNB()\n",
        "nb_clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_nb = nb_clf.predict(X_test_tfidf)\n",
        "print(\"Naive Bayes Classification Report:\\n\", classification_report(y_test, y_pred_nb))\n",
        "print(\"Naive Bayes Accuracy:\", accuracy_score(y_test, y_pred_nb))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vt3cKhThLpCn"
      },
      "source": [
        "### **Pre-trained Models**\n",
        "For advanced analysis, we used transformer-based models like finBERT with libraries such as `transformers` from Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30873, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the FinBERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
        "model_bert = BertForSequenceClassification.from_pretrained(\"yiyanghkust/finbert-tone\", num_labels=3)\n",
        "model_bert.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('yiyanghkust/finbert-pretrain')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>created_at</th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>Text_Cleaned</th>\n",
              "      <th>tokens</th>\n",
              "      <th>n_word</th>\n",
              "      <th>vader_score</th>\n",
              "      <th>vader_sentiment</th>\n",
              "      <th>sentiment_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>77522</td>\n",
              "      <td>2020-04-15 01:03:46+00:00</td>\n",
              "      <td>RT @RobertBeadles: YoðŸ’¥\\nEnter to WIN 1,000 Mon...</td>\n",
              "      <td>positive</td>\n",
              "      <td>robertbeadles yo enter win monarch tokens us c...</td>\n",
              "      <td>[robertbeadles, yo, enter, win, monarch, token...</td>\n",
              "      <td>14</td>\n",
              "      <td>0.5859</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>661634</td>\n",
              "      <td>2020-06-25 06:20:06+00:00</td>\n",
              "      <td>#SriLanka surcharge on fuel removed!\\nâ›½ðŸ“‰\\nThe ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>srilanka surcharge fuel removed surcharge rs i...</td>\n",
              "      <td>[srilanka, surcharge, fuel, removed, surcharge...</td>\n",
              "      <td>26</td>\n",
              "      <td>0.2023</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>413231</td>\n",
              "      <td>2020-06-04 15:41:45+00:00</td>\n",
              "      <td>Net issuance increases to fund fiscal programs...</td>\n",
              "      <td>positive</td>\n",
              "      <td>net issuance increases fund fiscal programs gt...</td>\n",
              "      <td>[net, issuance, increases, fund, fiscal, progr...</td>\n",
              "      <td>25</td>\n",
              "      <td>0.0516</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>760262</td>\n",
              "      <td>2020-07-03 19:39:35+00:00</td>\n",
              "      <td>RT @bentboolean: How much of Amazon's traffic ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>bentboolean much amazons traffic served fastly...</td>\n",
              "      <td>[bentboolean, much, amazons, traffic, served, ...</td>\n",
              "      <td>14</td>\n",
              "      <td>0.3818</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>830153</td>\n",
              "      <td>2020-07-09 14:39:14+00:00</td>\n",
              "      <td>$AMD Ryzen 4000 desktop CPUs looking â€˜greatâ€™ a...</td>\n",
              "      <td>positive</td>\n",
              "      <td>ryzen desktop cpus looking great track launch ...</td>\n",
              "      <td>[ryzen, desktop, cpus, looking, great, track, ...</td>\n",
              "      <td>10</td>\n",
              "      <td>0.6249</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id                 created_at  \\\n",
              "0   77522  2020-04-15 01:03:46+00:00   \n",
              "1  661634  2020-06-25 06:20:06+00:00   \n",
              "2  413231  2020-06-04 15:41:45+00:00   \n",
              "3  760262  2020-07-03 19:39:35+00:00   \n",
              "4  830153  2020-07-09 14:39:14+00:00   \n",
              "\n",
              "                                                text sentiment  \\\n",
              "0  RT @RobertBeadles: YoðŸ’¥\\nEnter to WIN 1,000 Mon...  positive   \n",
              "1  #SriLanka surcharge on fuel removed!\\nâ›½ðŸ“‰\\nThe ...  negative   \n",
              "2  Net issuance increases to fund fiscal programs...  positive   \n",
              "3  RT @bentboolean: How much of Amazon's traffic ...  positive   \n",
              "4  $AMD Ryzen 4000 desktop CPUs looking â€˜greatâ€™ a...  positive   \n",
              "\n",
              "                                        Text_Cleaned  \\\n",
              "0  robertbeadles yo enter win monarch tokens us c...   \n",
              "1  srilanka surcharge fuel removed surcharge rs i...   \n",
              "2  net issuance increases fund fiscal programs gt...   \n",
              "3  bentboolean much amazons traffic served fastly...   \n",
              "4  ryzen desktop cpus looking great track launch ...   \n",
              "\n",
              "                                              tokens  n_word  vader_score  \\\n",
              "0  [robertbeadles, yo, enter, win, monarch, token...      14       0.5859   \n",
              "1  [srilanka, surcharge, fuel, removed, surcharge...      26       0.2023   \n",
              "2  [net, issuance, increases, fund, fiscal, progr...      25       0.0516   \n",
              "3  [bentboolean, much, amazons, traffic, served, ...      14       0.3818   \n",
              "4  [ryzen, desktop, cpus, looking, great, track, ...      10       0.6249   \n",
              "\n",
              "  vader_sentiment  sentiment_encoded  \n",
              "0        positive                  2  \n",
              "1        positive                  2  \n",
              "2        positive                  2  \n",
              "3        positive                  2  \n",
              "4        positive                  2  "
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tweet_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess the data\n",
        "tweets = tweet_data[\"Text_Cleaned\"].values\n",
        "labels = tweet_data[\"sentiment_encoded\"].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize the tweets\n",
        "def encode_tweets(tweets, tokenizer, max_length=128):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for tweet in tweets:\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            tweet,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        input_ids.append(encoded[\"input_ids\"])\n",
        "        attention_masks.append(encoded[\"attention_mask\"])\n",
        "\n",
        "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode tweets\n",
        "input_ids, attention_masks = encode_tweets(tweets, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split into train and test sets\n",
        "train_inputs, test_inputs, train_masks, test_masks, train_labels, test_labels = train_test_split(\n",
        "    input_ids, attention_masks, labels, test_size=0.2, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Shokoufeh\\AppData\\Local\\Temp\\ipykernel_6024\\217390848.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_inputs = torch.tensor(train_inputs)\n",
            "C:\\Users\\Shokoufeh\\AppData\\Local\\Temp\\ipykernel_6024\\217390848.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_inputs = torch.tensor(test_inputs)\n",
            "C:\\Users\\Shokoufeh\\AppData\\Local\\Temp\\ipykernel_6024\\217390848.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_masks = torch.tensor(train_masks)\n",
            "C:\\Users\\Shokoufeh\\AppData\\Local\\Temp\\ipykernel_6024\\217390848.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_masks = torch.tensor(test_masks)\n"
          ]
        }
      ],
      "source": [
        "# Convert to PyTorch tensors\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "test_inputs = torch.tensor(test_inputs)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "test_masks = torch.tensor(test_masks)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "test_labels = torch.tensor(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create DataLoaders\n",
        "batch_size = 16\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define optimizer and scheduler\n",
        "optimizer = torch.optim.AdamW(model_bert.parameters(), lr=2e-5, eps=1e-8)\n",
        "epochs = 4\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "def train():\n",
        "    model_bert.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            b_labels = batch[2].to(device).long()  # Convert labels to LongTensor\n",
        "\n",
        "            model_bert.zero_grad()\n",
        "            outputs = model_bert(b_input_ids, attention_mask=b_input_mask)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        print(f\"Epoch {epoch + 1}/{epochs} | Loss: {total_loss / len(train_dataloader)}\")\n",
        "        scheduler.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate():\n",
        "    model_bert.eval()\n",
        "    predictions, true_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            b_labels = batch[2].to(device)\n",
        "\n",
        "            outputs = model_bert(b_input_ids, attention_mask=b_input_mask)\n",
        "            logits = outputs.logits\n",
        "            predictions.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "            true_labels.append(b_labels.cpu().numpy())\n",
        "\n",
        "    predictions = np.concatenate(predictions)\n",
        "    true_labels = np.concatenate(true_labels)\n",
        "    print(\"Accuracy:\", accuracy_score(true_labels, predictions))\n",
        "    print(classification_report(true_labels, predictions, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/4 | Loss: 1.0540175390243531\n",
            "Epoch 2/4 | Loss: 0.586809101819992\n",
            "Epoch 3/4 | Loss: 0.31523855816572904\n",
            "Epoch 4/4 | Loss: 0.12900617661699654\n",
            "Accuracy: 0.784\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.65      0.64      0.65       161\n",
            "     Neutral       0.84      0.75      0.79       391\n",
            "    Positive       0.79      0.87      0.83       448\n",
            "\n",
            "    accuracy                           0.78      1000\n",
            "   macro avg       0.76      0.75      0.75      1000\n",
            "weighted avg       0.79      0.78      0.78      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run training and evaluation\n",
        "train()\n",
        "evaluate()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM03fad5izAbjAJ+sU1sFET",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
